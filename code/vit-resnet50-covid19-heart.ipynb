{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.0 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.40.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,WeightedRandomSampler, random_split, SubsetRandomSampler, Subset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# Torch Vision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "# Transformers\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import List\n",
    "# Train Test Evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "# Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing The Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g_vit_patch = 'google/vit-base-patch16-224'\n",
    "base_path = './data' # For training Data\n",
    "base_output_path = './output' # For Storing weights and all\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 15\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/ec2-user/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 278MB/s]\n"
     ]
    }
   ],
   "source": [
    "# CNN Model with Explicit Device Handling\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, device, cnn_model=models.resnet50(weights=models.ResNet50_Weights.DEFAULT)):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.cnn = cnn_model\n",
    "        num_ftrs = self.cnn.fc.in_features\n",
    "        self.cnn.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is on the correct device\n",
    "        x = x.to(self.device)\n",
    "        return self.cnn(x)\n",
    "\n",
    "# ViT Model with Explicit Device Handling\n",
    "class ViTModel(nn.Module):\n",
    "    def __init__(self, num_classes, device, vit_model_name=g_vit_patch):\n",
    "        super(ViTModel, self).__init__()\n",
    "        self.feature_extractor = ViTImageProcessor.from_pretrained(vit_model_name,do_rescale=False)\n",
    "        self.model = ViTForImageClassification.from_pretrained(\n",
    "            vit_model_name,\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True,\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is on the correct device\n",
    "        x = x.to(self.device)  # Move to the specified device\n",
    "        # Prepare inputs for ViT model\n",
    "        feature_input = self.feature_extractor(x, return_tensors=\"pt\").to(self.device)\n",
    "        logits = self.model(**feature_input).logits\n",
    "        return logits\n",
    "\n",
    "# Ensemble Model with Explicit Device Handling\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, num_classes, cnn_model, vit_model, device):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.vit_model = vit_model\n",
    "        self.fc = nn.Linear(num_classes * 2, num_classes)  # Final linear layer\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is on the correct device\n",
    "        x = x.to(self.device)\n",
    "        \n",
    "        # Forward through CNN and ViT\n",
    "        cnn_output = self.cnn_model(x)  # CNN output\n",
    "        vit_output = self.vit_model(x)  # ViT output\n",
    "        \n",
    "        # Combine outputs\n",
    "        combined_output = torch.cat((cnn_output, vit_output), dim=1)  # Concatenate\n",
    "        \n",
    "        # Final linear output\n",
    "        return self.fc(combined_output)\n",
    "    \n",
    "    def eval(self, *args, **kwargs):\n",
    "        super().eval(*args, **kwargs)  # Call eval() on the parent class\n",
    "        \n",
    "        # Call eval() on the underlying models\n",
    "        self.cnn_model.eval(*args, **kwargs)\n",
    "        self.vit_model.eval(*args, **kwargs)\n",
    "        \n",
    "    def train(self, *args, **kwargs):\n",
    "        super().train( *args, **kwargs)  # Call train() on the parent class\n",
    "        \n",
    "        # Call train() on the underlying models\n",
    "        self.cnn_model.train( *args, **kwargs)\n",
    "        self.vit_model.train( *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(base_path: str, device: torch.device):\n",
    "    # Define transformations\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        # Add more transformations if needed\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    print(\"Loading Datasets...\")\n",
    "    # Load datasets\n",
    "    full_dataset = datasets.ImageFolder(\n",
    "        root=base_path,\n",
    "        transform=train_transform\n",
    "    )\n",
    "\n",
    "    # Calculate the train-test split\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoader for train and test sets\n",
    "    train_loader = DataLoader(train_dataset, num_workers=4, batch_size=80, shuffle=True, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, num_workers=4, batch_size=80, shuffle=False, pin_memory=True)\n",
    "\n",
    "    print(\"All Done...\")\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Datasets...\n",
      "All Done...\n"
     ]
    }
   ],
   "source": [
    "# Define the folder where your ECG image data is stored\n",
    "data_folder = os.path.join(base_path, 'image-data-cardiac-process')\n",
    "\n",
    "train_loader, test_loader = preprocess(data_folder, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = len(train_loader.dataset.dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Quality Checker\n",
    "def check_sample_data(data_loader, expected_shape, num_samples=5):\n",
    "    # Get a few samples from the DataLoader\n",
    "    samples_checked = 0\n",
    "    for inputs, labels in data_loader:\n",
    "        if samples_checked >= num_samples:\n",
    "            break\n",
    "\n",
    "        # Check dimensions\n",
    "        if inputs.shape[1:] != expected_shape:\n",
    "            raise ValueError(f\"Unexpected input shape. Expected {expected_shape}, but got {inputs.shape[1:]}\")\n",
    "        \n",
    "        # Check data type\n",
    "        if inputs.dtype != torch.float32:\n",
    "            raise ValueError(f\"Unexpected data type. Expected torch.float32, but got {inputs.dtype}\")\n",
    "        \n",
    "        # Check normalization (values within range)\n",
    "        if inputs.min() < -1.0 or inputs.max() > 1.0:\n",
    "            raise ValueError(f\"Input data out of expected range. Values should be between -1 and 1.\")\n",
    "\n",
    "        # Check if labels are within the expected range\n",
    "        if torch.min(labels) < 0 or torch.max(labels) >= len(data_loader.dataset.dataset.classes):\n",
    "            raise ValueError(f\"Unexpected label range. Labels should be between 0 and {len(data_loader.dataset.dataset.classes) - 1}\")\n",
    "\n",
    "        samples_checked += 1\n",
    "\n",
    "    print(f\"Data check completed. {num_samples} samples checked successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking training data...\n",
      "Data check completed. 5 samples checked successfully.\n",
      "Checking test data...\n",
      "Data check completed. 5 samples checked successfully.\n"
     ]
    }
   ],
   "source": [
    "expected_shape = (3, 224, 224)\n",
    "print(\"Checking training data...\")\n",
    "check_sample_data(train_loader, expected_shape)\n",
    "\n",
    "print(\"Checking test data...\")\n",
    "check_sample_data(test_loader, expected_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EpochEvaluator:\n",
    "    def __init__(self, total_data_count: int, curr_epoch: int, total_epochs: int):\n",
    "        self.running_loss = 0.0\n",
    "        self.correct_preds = 0\n",
    "        self.total_preds = 0\n",
    "        self.total_data_count = total_data_count\n",
    "        self.curr_epoch = curr_epoch\n",
    "        self.total_epochs = total_epochs\n",
    "\n",
    "    def update_loss(self, loss: float):\n",
    "        self.running_loss += loss\n",
    "        return self\n",
    "\n",
    "    def update_preds(self, correct_preds: int, total_preds: int):\n",
    "        self.correct_preds += correct_preds\n",
    "        self.total_preds += total_preds\n",
    "        return self\n",
    "\n",
    "    def metrics(self):\n",
    "        epoch_loss = self.running_loss / self.total_data_count\n",
    "        epoch_accuracy = self.correct_preds / self.total_preds * 100\n",
    "        return epoch_loss, epoch_accuracy\n",
    "\n",
    "    def print_metrics(self):\n",
    "        epoch_loss, epoch_accuracy = self.metrics()\n",
    "        print(f'Epoch [{self.curr_epoch + 1}/{self.total_epochs}], '\n",
    "              f'Training Loss: {epoch_loss:.4f}, '\n",
    "              f'Training Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "class TrainEvaluator:\n",
    "    def __init__(self):\n",
    "        self.epochs = []\n",
    "\n",
    "    def add(self, epoch: EpochEvaluator):\n",
    "        self.epochs.append(epoch)\n",
    "\n",
    "    def plot_graph(self):\n",
    "        epoch_nums = [e.curr_epoch + 1 for e in self.epochs]\n",
    "        losses = [e.metrics()[0] for e in self.epochs]\n",
    "        accuracies = [e.metrics()[1] for e in self.epochs]\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epoch_nums, losses, 'o-', label='Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss vs Epoch')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epoch_nums, accuracies, 'o-', label='Training Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Training Accuracy vs Epoch')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer on the same device\n",
    "def train_model(model, train_loader, num_epochs, learning_rate):\n",
    "    train_eval = TrainEvaluator()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    total_train = len(train_loader)\n",
    "    print(f\"{num_epochs=} {total_train=}\")\n",
    "    with tqdm(total=num_epochs * total_train) as pbar:\n",
    "        for epoch in tqdm(range(num_epochs), position=0, leave=True):\n",
    "            model.train()  # Set model to training mode\n",
    "            epoch_eval = EpochEvaluator(total_train, epoch, num_epochs)\n",
    "            # Iterate through training data\n",
    "            for inputs, labels in tqdm(train_loader,total=total_train, position=0, leave=True):\n",
    "                inputs,labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "                outputs = model(inputs)  # Forward pass\n",
    "\n",
    "                loss = criterion(outputs, labels)  # Calculate loss\n",
    "                loss.backward()  # Backpropagation\n",
    "                optimizer.step()  # Update model parameters\n",
    "                pbar.update(1)\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                epoch_eval.update_loss(\n",
    "                    loss.item()\n",
    "                ).update_preds(\n",
    "                    (predicted == labels).sum().item(),labels.size(0)\n",
    "                )\n",
    "            epoch_eval.print_metrics()\n",
    "            train_eval.add(epoch_eval)\n",
    "    return train_eval\n",
    "            \n",
    "def test_model(model, test_loader):\n",
    "    # Evaluate the model to get accuracy, F1 score, and ROC AUC\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    # Initialize lists to store predictions and probabilities\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_proba = []\n",
    "\n",
    "    total_test = len(test_loader)\n",
    "\n",
    "    # Collect predictions and ground truth\n",
    "    with torch.no_grad():        \n",
    "        # Iterate through test data\n",
    "        for inputs, labels in tqdm(test_loader, total=total_test):\n",
    "            inputs,labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "            # Extend lists with values from current batch\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_proba.extend(probs.cpu().numpy())  # Get probabilities for ROC AUC\n",
    "\n",
    "    # Reshape y_proba into a 2D array\n",
    "    y_proba = np.array(y_proba)\n",
    "    return y_true, y_pred, y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the trained models to disk\n",
    "def save_models(base_path:str, *models):\n",
    "    if not os.path.exists(base_path):\n",
    "        os.makedirs(base_path)\n",
    "    # Save models to disk\n",
    "    for model in models:\n",
    "        torch.save(model.state_dict(), os.path.join(base_path, model.__class__.__name__))\n",
    "    print(\"Models saved successfully!\")\n",
    "    \n",
    "\n",
    "\n",
    "def load_model_with_weights_if_exists(model, base_path:str):\n",
    "    model_name = model.__class__.__name__\n",
    "    model_path = os.path.join(base_path, model_name)\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        print('model not found: ', model_path, model_name)\n",
    "    return model\n",
    "    \n",
    "# Load the trained models from disk\n",
    "def load_models(device:torch.device, base_path:str):\n",
    "    # Initialize models\n",
    "    num_classes = 6\n",
    "    cnn_model = load_model_with_weights_if_exists(CNNModel(num_classes,device))\n",
    "    vit_model = load_model_with_weights_if_exists(ViTModel(num_classes,device))\n",
    "    ensemble_model = load_model_with_weights_if_exists(EnsembleModel(num_classes, cnn_model, vit_model, device))\n",
    "    \n",
    "    # Move models to the GPU if available\n",
    "    cnn_model = cnn_model.to(device)\n",
    "    vit_model = vit_model.to(device)\n",
    "    ensemble_model = ensemble_model.to(device)\n",
    "\n",
    "    return cnn_model, vit_model, ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2394d24dbac47f093a7f1a8b087684e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfd8bef0394482f8b7f63d3180a9687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd9f07bcbf14cf89a6f812eede5349d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs=15 total_train=637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [20:11<00:00,  1.90s/it]it]\n",
      "  7%|▋         | 1/15 [20:11<4:42:40, 1211.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Training Loss: 0.3492, Training Accuracy: 88.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [19:51<00:00,  1.87s/it]/it]\n",
      " 13%|█▎        | 2/15 [40:03<4:19:57, 1199.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Training Loss: 0.2386, Training Accuracy: 91.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [19:50<00:00,  1.87s/it]/it]\n",
      " 20%|██        | 3/15 [59:53<3:59:04, 1195.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/15], Training Loss: 0.2020, Training Accuracy: 92.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [19:49<00:00,  1.87s/it]0s/it]\n",
      " 27%|██▋       | 4/15 [1:19:42<3:38:42, 1192.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/15], Training Loss: 0.1656, Training Accuracy: 94.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [19:47<00:00,  1.86s/it]2s/it]\n",
      " 33%|███▎      | 5/15 [1:39:29<3:18:28, 1190.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/15], Training Loss: 0.1257, Training Accuracy: 95.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [19:49<00:00,  1.87s/it]0s/it]\n",
      " 40%|████      | 6/15 [1:59:18<2:58:33, 1190.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/15], Training Loss: 0.0872, Training Accuracy: 97.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [19:45<00:00,  1.86s/it]8s/it]\n",
      " 47%|████▋     | 7/15 [2:19:04<2:38:31, 1188.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/15], Training Loss: 0.0606, Training Accuracy: 98.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [19:51<00:00,  1.87s/it]5s/it]\n",
      " 53%|█████▎    | 8/15 [2:38:56<2:18:47, 1189.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/15], Training Loss: 0.0481, Training Accuracy: 98.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [19:46<00:00,  1.86s/it]9s/it]\n",
      " 60%|██████    | 9/15 [2:58:42<1:58:51, 1188.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/15], Training Loss: 0.0436, Training Accuracy: 98.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [19:49<00:00,  1.87s/it]5s/it]\n",
      " 67%|██████▋   | 10/15 [3:18:32<1:39:04, 1188.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/15], Training Loss: 0.0335, Training Accuracy: 98.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [19:48<00:00,  1.87s/it]2s/it]\n",
      " 73%|███████▎  | 11/15 [3:38:20<1:19:14, 1188.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/15], Training Loss: 0.0322, Training Accuracy: 99.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [19:40<00:00,  1.85s/it]/it]  \n",
      " 80%|████████  | 12/15 [3:58:01<59:18, 1186.32s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/15], Training Loss: 0.0292, Training Accuracy: 99.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 428/637 [13:16<06:29,  1.86s/it]/it]"
     ]
    }
   ],
   "source": [
    "# Initialize the models\n",
    "cnn_model = CNNModel(num_classes,device)\n",
    "vit_model = ViTModel(num_classes,device, g_vit_patch)\n",
    "ensemble_model = EnsembleModel(num_classes, cnn_model, vit_model, device)\n",
    "\n",
    "# Move models to the GPU if available\n",
    "vit_model = vit_model.to(device)\n",
    "vit_model = vit_model.to(device)\n",
    "ensemble_model = ensemble_model.to(device)\n",
    "\n",
    "model = ensemble_model\n",
    "\n",
    "eval = train_model(model, train_loader, num_epochs, learning_rate=9e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot accuracy and loss for each fold\n",
    "eval.plot_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export models\n",
    "trained_model_path = os.path.join(base_output_path, \"model_weights_vit_resnet_v1_clean_13ep\")\n",
    "save_models(trained_model_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnsembleModel(num_classes, cnn_model, vit_model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true, y_pred, y_proba = test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true_binary = label_binarize(y_true, classes=np.arange(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true == y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Calculate F1 score (weighted to account for class imbalance)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate ROC curve and AUC for each class using scikit-learn\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve((np.array(y_true) == i).astype(int), y_proba[:, i])\n",
    "    roc_auc[i] = roc_auc_score((np.array(y_true) == i).astype(int), y_proba[:, i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC AUC\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(np.eye(num_classes)[np.array(y_true)].ravel(), y_proba.ravel())\n",
    "roc_auc[\"micro\"] = roc_auc_score(np.eye(num_classes)[np.array(y_true)], y_proba, average=\"micro\")\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey', label='Random Guess', alpha=0.5)\n",
    "for i in range(num_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.2f})', linestyle=':', linewidth=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Multi-Class Classification')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print average ROC AUC\n",
    "print(\"Average ROC AUC:\", roc_auc[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print accuracy and F1 score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Confusion matrix for additional visualization\n",
    "confusion_mtx = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues', xticklabels=test_loader.dataset.dataset.classes, yticklabels=test_loader.dataset.dataset.classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5005588,
     "sourceId": 8410525,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
